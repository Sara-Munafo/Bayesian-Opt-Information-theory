{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9567d23d",
   "metadata": {},
   "source": [
    "# TO ADD IN COMPLETE NOTEBOOK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c94aa3",
   "metadata": {},
   "source": [
    "## B.2. Sequential Monte Carlo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6845b25",
   "metadata": {},
   "source": [
    "**Parameters:**\n",
    "theta = [$\\theta_0,\\theta_i$], $i=1,..,d$ $\\rightarrow$ d+1 total hyperparameters\n",
    "- $\\theta_0$ is the square root of the multiplication constant of the kernel\n",
    "- $\\theta_i$ ($i=1,..d$) are the length scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec623e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best paramet4ers from max k=likelihood\n",
    "# 184**2 * Matern(length_scale=[21.1, 22.1], nu=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e187fc",
   "metadata": {},
   "source": [
    "### Sequential Monte-Carlo    \n",
    "\n",
    "In SMC we iteratively update a sample of size N, using information from the data to approximate better the posterior.    \n",
    "The basic Idea is the following:\n",
    " 1. Obtain a sequence of samples of theta distributed approximately as the posterior\n",
    " 2. average the acquisition function over those samples\n",
    " \n",
    "SMC takes care of the first point. If we want to sample from the posterior distribution $p(\\theta,D_n)$, this distribution is not easily integrabel and the dependencies on $\\theta$ are not trivial.    \n",
    "We can then make use of the so called **Importance Sampling** technique.    \n",
    "We approximate $$p(\\theta|D_n) \\simeq w(\\theta) \\pi(\\theta|D_n)$$\n",
    "where\n",
    "- $\\pi(\\theta|D_n)$ importance sampling density: can have different forms, often a delta distribution;\n",
    "- $w(\\theta)$ : importance sampling weight: associates an 'importance'to each value of $\\theta$ based on maximum likelihood \n",
    "With this approximation, we can write: $$E_{\\theta|D_n}[\\alpha(x,\\theta)]=\\int \\alpha(x,theta)p(\\theta|D_n)d\\theta \\simeq \\sum_{i=1}^N \\alpha(\\theta^{(i)})w_t^{(i)}$$\n",
    "\n",
    "(averaged over $N$ samples of the hyperparameters).\n",
    "The goodness of this algorithm resides on taking a good sample of hyperparameters, which is therefore determined by the appropriate weights. Here follows the algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1be09b",
   "metadata": {},
   "source": [
    "Given:\n",
    "- $p(\\theta_0)$ : prior of hyperparameters\n",
    "- $p(\\theta_t|\\theta_{t-1})$ : prior of transition probabilities\n",
    "- $p(y_t|\\theta)=\\dfrac{e^{-\\dfrac{1}{2}(y_t K(\\theta)^{-1})y_t}}{\\sqrt{|2\\pi K(\\theta)|}}$  likelihood of inferred $y_t$ given a set of hyperparameters\n",
    "\n",
    "We proceed as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc184be8",
   "metadata": {},
   "source": [
    "> 1. Before training: initialize a sample of size N (same size of x and y) of hyperparameters according to the prior distribution    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5bd0c1",
   "metadata": {},
   "source": [
    "> 2. For each iteration t of training:    \n",
    "    >- For $i=0,...,N-1$    \n",
    "        > #add a new sample of N hyperparameters, one by one    \n",
    "        >$\\theta_t^{(i)}\\sim p(\\theta_t^{(i)}|\\theta_{t-1}^{(i)}$    \n",
    "        \n",
    ">$\\theta_{0:t}=[\\theta_{0:t-1},\\theta_t]$   \n",
    "    >- For $i=0,...N-1$:    \n",
    "    >#compute weights with likelihood    \n",
    "        >$w_t^{(i)}=p(y_t|\\theta_t^{(i)})$       ($y_t$ is the best y obtained in the training iteration)    \n",
    "        >Substitute the 'layer' t of hyperparameters by resampling N hyperparameters (one by one) using the weights\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64ada39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimension of parameters space\n",
    "k_dim = len(x[0])+1\n",
    "\n",
    "T = 50\n",
    "N = 100\n",
    "\n",
    "theta = np.zeros((N,k_dim,T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e6e9bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best values from max likelihood\n",
    "theta_opt = [184, 21.1, 22.1]\n",
    "\n",
    "#variances of Gaussians\n",
    "var = [184/100, 21.1/100, 22.1/100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf863b68",
   "metadata": {},
   "source": [
    "**Note on the function below**:\n",
    "the kernel has to be a semidefinite positive matrix: if one of the hyperparameters is $<0$ it doesn't work, it returns nan and it doesn't do anything;  the problem is that, since we choose theta at each $t$ from a gaussian which can be centered anywhere, we may also have negative values; The problem is solved if we don't start with zero mean gaussians and we take a small variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2801e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smc(x,y,N,k_dim,T,theta_opt, var):\n",
    "    '''Initialization'''\n",
    "    theta = np.zeros((N,k_dim,T))\n",
    "    print(theta.shape)\n",
    "    theta[:,:,0] = np.full((N,k_dim), 10) #to avoid negative theta we start far from 0\n",
    "    w = np.zeros((N,T))\n",
    "    \n",
    "    \n",
    "    for t in range(1, T):\n",
    "        for i in range(N):\n",
    "            for j in range(k_dim):\n",
    "                theta[i,j,t] = np.random.normal(loc=theta[i,j,t-1], scale=var[j])\n",
    "                \n",
    "            #compute weights (gp needs to be computed with each set of hyperpars)\n",
    "            kernel = (theta[i,0,t]**2) * Matern(length_scale=[theta[i,1,t],theta[i,2,t]], nu=1.5)\n",
    "            gp = GaussianProcessRegressor(kernel=kernel, optimizer=None, alpha=1e-5)\n",
    "            gp.fit(x,y)\n",
    "            #print(gp.log_marginal_likelihood())\n",
    "            w[i,t] = np.exp(gp.log_marginal_likelihood())\n",
    "        #print(w[:,t])\n",
    "        #normalize weights\n",
    "        w[:,t]/=np.sum(w[:,t])\n",
    "        \n",
    "        '''start resampling'''\n",
    "        #print(w[:,t])\n",
    "        #resample with replacement:\n",
    "        for i in range(N):\n",
    "            index = np.random.choice(N, size=1, p=w[:,t])\n",
    "            theta[i,:,t] = theta[index,:,t]\n",
    "        \n",
    "    theta_best = np.mean(theta[:,:,T-1], axis=0)\n",
    "    return theta_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "05d75925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3, 50)\n"
     ]
    }
   ],
   "source": [
    "theta_smc = smc(sample_x,sample_y,N,k_dim,T,theta_opt,var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "af602238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([71.26854205,  7.93198393,  7.68689949])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_smc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3944152f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.29469807943873\n"
     ]
    }
   ],
   "source": [
    "#LetÅ› see\n",
    "kernel = (theta_smc[0]**2) * Matern(length_scale=[theta_smc[1],theta_smc[2]], nu=1.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, optimizer=None, alpha=1e-5)\n",
    "gp.fit(x,y)\n",
    "print(gp.log_marginal_likelihood())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ee185e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-500.65767685994564\n"
     ]
    }
   ],
   "source": [
    "#while the initial value was:\n",
    "kernel = (10**2) * Matern(length_scale=[10,10], nu=1.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, optimizer=None, alpha=1e-5)\n",
    "gp.fit(x,y)\n",
    "print(gp.log_marginal_likelihood())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfafac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15476eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9000c4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
